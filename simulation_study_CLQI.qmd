
This qmd is a modified version of `one_basis_study_scenerio.qmd` that cleaned up everything so we can get results for the simulation study. We will be testing 8 scenerios, all with one missing study and one basis study

- Two sample sizes (n=500, n=1500)
- Two LD ranges (~30%, ~60%)
- Two levels of heterogeneity (Small: 0.01, Large: 0.1)

My hope is that this qmd is easily readable. If not, please contact me: djang26@amherst.edu

```{r}
#necessary libraries
library(tidyverse)
library(quantreg)
library(MASS)
library(purrr)
library(tictoc)
```

These are necessary transformation functions that we will be using throughout this study

```{r}
#logit and expit functions for myself
logit <- function(prob) {
  value <- log(prob / (1 - prob))
  return(value)
}

expit <- function(prob) {
  value <- 1 / (1 + exp(-(prob)))
  return(value)
}

#now some other transformation functions
log_quant_transform <- function(value, min, max) {
  new_value <- log((value - min) / (max - value))
  
  if (is.nan(new_value)) {return(NA)} #negative log values return an NA. Will be important to address
  else {return(new_value)}
}

inv_log_quant_transform <- function(value, min, max) {
  new_value <- (exp(value)*max + min) / (1+exp(value))
  return(new_value)
}
```

`coefficient_generator` generates coefficients for our two studies to be generated out of
Tau represents our heterogenity among studies. The higher the value, the more variable the coefficients are between each iteration
We chose the point values for the coefficients based off of the previous paper introducing CQI for consistency

This function outputs a dataset including all the necessary information from which the distributions that the data are generated from are

```{r}
coefficient_generator <- function(tau, num_studies, 
                                  a_0 = logit(0.3), a_1 = log(1.1), 
                                  b_0 = logit(0.1), b_1 = log(1.5), b_2 = log(0.7), b_3 = log(1.2)) {
    
  #create which number study we have
  study_counts <- data.frame(id = 1:num_studies)
  
  study_counts <- study_counts |>
    mutate(study_num = paste("Study", id)) |>
    dplyr::select(study_num)
  
  #allow a_1 to vary
  alpha_coefficients <- rnorm(num_studies, a_1, tau)
  alpha_coefficients <- as.data.frame(alpha_coefficients) #make into data frame
  
      #renaming for better binding experience + adding a_0... not important here
      alpha_coefficients <- alpha_coefficients |> 
        mutate(a_0 = a_0) |>
        rename(a_1 = "alpha_coefficients") |>
        dplyr::select(a_0, a_1) #reordering
  
      
  #now doing the beta coefficients
  beta_vector <- c(b_1, b_2, b_3)
  
    #variance-covariance matrix
    matrix_size <- length(beta_vector)
    diag_mat <- matrix(0, matrix_size, matrix_size) 
    diag(diag_mat) <- 1 #make all the diagonals 1 for the identity matrix
    
    #lastly, we need to perform the calculation specified in section 3.1.4
    beta_matrix <- tau * diag_mat
    beta_coefficients <- mvrnorm(num_studies, beta_vector, beta_matrix)
    beta_coefficients <- as.data.frame(beta_coefficients)
    
      #renaming for better binding experience
      beta_coefficients <- beta_coefficients |> 
        mutate(b_0 = b_0) |>
        rename(b_1 = "V1", b_2 = "V2", b_3 = "V3") |>
        dplyr::select(b_0, b_1, b_2, b_3)
      
  #we need to set a LoD value for these studies that has slight variation
  LoD <- data.frame(LoD = round(runif(num_studies, min = 1.6, max = 2.4), 3))
  
  #lastly, we need a range of degrees of freedom for the confounder generation
  #there will be two just in case I want to use an F distribution
  deg_freedom <- data.frame(df_1 = round(runif(num_studies, min = 3, max = 7)),
                            df_2 = round(runif(num_studies, min = 3, max = 7)))
  
  #now combine these results
  coefficients <- cbind(study_counts, alpha_coefficients, beta_coefficients, LoD, deg_freedom)
  
  return(coefficients)
}
```

`create_data` creates a single dataset based on a given sample size and specified coefficients

```{r}
#this will be nested to make the list function cleaner to read
create_data <- function(sample_size, a_0, a_1, b_0, b_1, b_2, b_3, df_1, df_2) {
  
  #initialize everything
  n <- sample_size
  
  #step 1: Base Binary Predictor
  V <- rbinom(n, size = 1, prob = 0.4)
  
  #step 2: cnfounder with a skewed distribution... Biomarker missing
  C1 <- rchisq(n, df = df_1) 
  #C2 <- rnorm(n, mean = 75, sd = 7) #another confounder with a normal distribution. not used right now
  
  #step 3: generating exposure variable based on confounders (probability)
  E <- expit(a_0 + a_1*C1)
  E_bin <- rbinom(n, size = 1, prob = E)
  
  #step 4: generating outcome based on confounders, exposure, and base binary predictor
  O <- expit(b_0 + b_1*E + b_2*V + b_3*C1)
  O_bin <- rbinom(n, size = 1, prob = O)
  
  #step 5: create dataset
  
  my_data <- data.frame(
    predictor = V,
    confounder_1 = C1,
    exposure = E_bin,
    outcome = O_bin
  )
  
  return(my_data)
} 
```

`create_multiple_datasets` uses `create_data` to, well you guessed it, create multiple datasets. The distributions from which the data are generated for each dataset comes from information we get from `coefficient_generator`

```{r}
#recall all coefficients are stored in coefficient_generator funct
create_multiple_datasets <- function(study_coefficient_dataset, sample_size) {
  my_list <- list() #initialize empty list
  
  #pulling all values we need
  a_0_values <- study_coefficient_dataset |> dplyr::pull(a_0)
  a_1_values <- study_coefficient_dataset |> dplyr::pull(a_1)
  b_0_values <- study_coefficient_dataset |> dplyr::pull(b_0)
  b_1_values <- study_coefficient_dataset |> dplyr::pull(b_1)
  b_2_values <- study_coefficient_dataset |> dplyr::pull(b_2)
  b_3_values <- study_coefficient_dataset |> dplyr::pull(b_3)
  
  #also degrees of freedom
  df_1_values <- study_coefficient_dataset |> dplyr::pull(df_1)
  df_2_values <- study_coefficient_dataset |> dplyr::pull(df_2)
  
  for(i in 1:nrow(study_coefficient_dataset)) {
    #do the data generating mechanism
    a_0 <- a_0_values[i]
    a_1 <- a_1_values[i]
    b_0 <- b_0_values[i]
    b_1 <- b_1_values[i]
    b_2 <- b_2_values[i]
    b_3 <- b_3_values[i]
    
    #also the degrees of freedom
    df_1 <- df_1_values[i]
    df_2 <- df_2_values[i]
    
    #apply the data generating mechanism function from the values above
    my_data <- create_data(sample_size = sample_size, 
                           a_0 = a_0, 
                           a_1 = a_1, 
                           b_0 = b_0, 
                           b_1 = b_1, 
                           b_2 = b_2, 
                           b_3 = b_3,
                           df_1 = df_1,
                           df_2 = df_2)
    
    
    #finally, add this dataset to our list
    my_list[[i]] <- my_data
  
  }
  
  #return the list
  return(my_list)
}
```

Last but not least, `data_generating_mechanism` creates our datasets with the LD missingness added. We cannot extract the true coefficients from this function (to keep things simple), but with some modification, you can do that. 

*Warning: The LD should be chosen from this, which means I have to edit my functions later*

```{r}
data_generating_mechanism <- function(sample_size, LD, tau) {
  
  #first, create my studies based on coefficients
  my_coefficients <- coefficient_generator(tau = tau, num_studies = 2) #2 is consistent here
  my_studies <- create_multiple_datasets(my_coefficients, sample_size = sample_size)
  
  #log transform our data
  min = 0
  max = ceiling(quantile(my_studies[[1]]$confounder_1, prob = 0.99))
  
  my_studies[[1]] <- my_studies[[1]] |>
    mutate(confounder_1_transformed = sapply(confounder_1, log_quant_transform, min, max))
  
  #define the LD and make data missing based on that
  LoD <- my_coefficients[2,]$LoD
  
  #now we made the data missing based on our given LD, and also transform this value
  my_studies[[2]] <- my_studies[[2]] |>
    mutate(confounder_1_missing = ifelse(confounder_1 >= LoD, confounder_1, NA))
  
  #just to be able to look into my_coefficients later
  my_studies <- list(my_studies[[1]], my_studies[[2]], my_coefficients)
  return(my_studies) #return list of studies with missingness added
}
```

**************************************************************************************

Here are some more functions that will be running our algorithm, along with definitions on what exactly they do

`logistic_quantile_regression_coefficients` creates our set of regression coefficients from our *basis study* to send to the *missing study*. It outputs a dataset of the coefficients, ready for quantile regression

```{r}
logistic_quantile_regression_coefficients <- function(basis_study) {
 
  coefficient_data <- data.frame() #initiliaze empty data frame
  
  for(i in seq(from = 0.01, to = 0.99, by = 0.01)) {
    reg_coeff <- rq(confounder_1_transformed ~ exposure + predictor + outcome, data = basis_study, tau=i)
    
    new_data <- data.frame(
      b0 = reg_coeff$coefficients[1],
      b_exposure = reg_coeff$coefficients[2],
      b_predictor = reg_coeff$coefficients[3],
      b_outcome = reg_coeff$coefficients[4],
      quant = i)
    
    coefficient_data <- rbind(coefficient_data, new_data) #add to new iterations
  }
  return(coefficient_data)
}
```

`find_prop_missing` finds the proportion of LD data for the missing study. This will end up being useful. It will report a whole number

```{r}
#make this into a function for easy use?
find_prop_missing <- function(missing_data_study, LD) {
  prop_missing <- sum(is.na(missing_data_study[[LD]])) / nrow(missing_data_study)
  prop_missing <- ceiling(prop_missing * 100)
  
  return(prop_missing)
}
```

`uniform_values` generates a random value from a unif(1,99) distribution, then extracts the floor, ceiling, and modulus for the CLQI algorithm

```{r}
uniform_values <- function(specified_max) {
  u <- runif(1, min = 1, max = specified_max) #aka from 0.01 to 0.99
  
  #all the values we need from the uniform
  floor_u <- floor(u)
  mod_u <- (u - floor(u))
  next_u <- ceiling(u)
  
  #now putting this into a vector
  my_u <- c(u, floor_u, mod_u, next_u)
  return(my_u)
}
```

`fixed_MI` is the function that implements fixed MI. 
We can use tidyverse functionality to slightly modify the use of this function, *although that may be computationally inefficient*

```{r}
fixed_MI <- function(basis_coefficients, study_for_imputation, 
                     var_for_imputation, row_index, u_vector) {
  
  u_vector <- u_vector
  floor_quantile <- basis_coefficients[u_vector[2], ] #floor
  ceiling_quantile <- basis_coefficients[u_vector[4], ] #ceiling
  
  #need to calculate regression values... really messy don't look at this
  lower_quantile_value <- floor_quantile$b0 + (floor_quantile$b_predictor * study_for_imputation[row_index,]$predictor) +
    (floor_quantile$b_exposure * study_for_imputation[row_index,]$exposure) + (floor_quantile$b_outcome * study_for_imputation[row_index,]$outcome)

  upper_quantile_value <- ceiling_quantile$b0 + (ceiling_quantile$b_predictor * study_for_imputation[row_index,]$predictor) +
    (ceiling_quantile$b_exposure * study_for_imputation[row_index,]$exposure) + (ceiling_quantile$b_outcome * study_for_imputation[row_index,]$outcome)

  modulus <- u_vector[3]
  imputation_value_transformed <- ((1-modulus)*lower_quantile_value) + (modulus*upper_quantile_value)
  
  #lastly, untransform this value using the right min and max... organization is a headache
  min_imp <- 0
  missing_data_proportion <- find_prop_missing(missing_data_study = study_for_imputation, 
                                               LD = as.character(var_for_imputation)) / 100
  
  #THIS IS THE ISSUE...
  max_imp <- as.numeric(quantile(study_for_imputation$confounder_1, missing_data_proportion))
  #then normal distribution to add random error... AHHHHHHH
  
  #remember min is 0 and prop_missing gives us the right quantile for calculating max from study_for_imputation
  imputed_value_regular <- inv_log_quant_transform(value = imputation_value_transformed, 
                                                   min = min_imp, 
                                                   max = max_imp)
  
  #unif_val <- u_vector[1]
  #return(unif_val)
    
  return(imputed_value_regular)
}
```

`CLQI_algorithm` is the function that applies the previous few functions and imputes a unique imputed value to each missing observation. This returns a dataset with one full imputation, ready for checking

```{r}
#this is the CLQI algorithm, with some modifications to the u_vector idea
#for fixed MI, just move where the my_uniform_values gets called, so we use the same uniform value each time
#sadly this is not efficient, but for readability, and also for me to not go insane, this is what I did

CLQI_algorithm <- function(missing_dataset, basis_coefficients, specified_max) {

  missing_dataset$confounder_1_CLQI <- NA #initialize a new variable here... base R ugh
  
  for(row_index in 1:nrow(missing_dataset)) {
    if(is.na(missing_dataset$confounder_1_missing[row_index])) {
      my_uniform_values <- uniform_values(specified_max) #generate unique uniform values
      
      imputed_value <- fixed_MI(basis_coefficients = basis_coefficients, 
                                study_for_imputation = missing_dataset, 
                                var_for_imputation = "confounder_1_missing",
                                row_index = row_index, #to get the right one
                                u_vector = my_uniform_values)
      
      missing_dataset$confounder_1_CLQI[row_index] <- imputed_value
    }
    else {
      missing_dataset$confounder_1_CLQI[row_index] <- missing_dataset$confounder_1_missing[row_index]
    }
  }
  
  return(missing_dataset)
}
```

`CLQI_MI` applies the CLQI algorithm to the missing_dataset 10 times and combines the results according to rubin's rules

```{r}
CLQI_MI <- function(num_iterations = 10, missing_dataset, 
                    basis_coefficients, var_for_imputation,
                    specified_max) {
  
  b1_coefficients <- numeric(num_iterations) #initialize vector
  
  for(i in 1:num_iterations) {
    #run the algorithm
    missing_dataset_imputed <- CLQI_algorithm(missing_dataset, basis_coefficients, specified_max)
      
    #run logistic regression
    log_reg_result <- glm(outcome ~ exposure + predictor + confounder_1_CLQI, 
                          data = missing_dataset_imputed, 
                          family = "binomial")
      
    #extract exposure coefficients (b1)
    b1_coefficients[i] <- exp(log_reg_result$coefficients[2])
  }
  
  return(b1_coefficients)
}
  
```

**************************************************************************************

We will now perform Setting 1 of the simulation study. This will serve as a closer step-by-step guide on how the study is performed
- n = 500
- LD ~ 30%
- Small Heterogeneity (tau = 0.01)

```{r}
set.seed(605) #set seed for reproducibility

my_sample_size = 500
my_tau = 0.01
```

Recall that we need to run the data generating mechanism for each simulation iteration, as well as proceed with the CLQI algorithm, Fixed MI, constant imputation, and complete case (CC) analysis. 

The workflow will look as follows *for each iteration*:
- Generate data with new set of coefficients each time
>> Remember to extract the list with basis_study and missing_study so code works later on...

- Run the conditional logistic quantile regressions to extract regression coefficients to "send over" to missing study
- Missing Study performs CLQI, as well as the other imputation methods (fixed MI, constant)
- Do logistic regression to get b1 coefficient for each method. Don't forget Complete Case (CC)
>> Don't forget to do the MI thing for CLQI, repeating the algorithm 10 times so that we can get an aggregate result
>> This also applies to fixed MI. We must do it 10 times

- Store all necessary information for bias, MSE, coverage, etc into a new dataset
- Rinse and repeat 1000 times for the simulation setting


**************************************************************************************

This function will provide one iteration of the simulation, utilizing most, if not all, the functions we defined previously

```{r}
specified_sample_size <- 500
specified_tau <- 0.01

one_iteration <- function(specified_sample_size, specified_tau) {
  #generate the data
  my_data <- data_generating_mechanism(sample_size = specified_sample_size, 
                                       LD = 0, 
                                       tau = specified_tau)
  
  basis_study <- as.data.frame(my_data[[1]])
  missing_study <- as.data.frame(my_data[[2]])
  
  #the basis dataset runs conditional logistic quantile regression
  basis_regression_coefficents <- logistic_quantile_regression_coefficients(basis_study)
  prop <- find_prop_missing(missing_study, "confounder_1_missing")
  
  
  CLQI_MI_results <- CLQI_MI(num_iterations = 10, 
                             missing_dataset = missing_study, 
                             basis_coefficients = basis_regression_coefficents, 
                             var_for_imputation = "ooga",
                             specified_max = prop) 
  
  
  
  return(CLQI_MI_results)
}

a <- one_iteration(specified_sample_size = 500, specified_tau = 0.01)
mean(a)
```





**************************************************************************************





```{r}
#check the runtime here
tictoc::tic()
CLQI_MI_results <- CLQI_MI(missing_dataset = missing_study, 
                           basis_coefficients = coefficient_data,
                           var_for_imputation = "confounder_1_missing")
tictoc::toc()
```

`STEP 9`: Aggregate with Rubinâ€™s Rules and calculate necessary values

```{r}
#recall that the true value of the coefficient is exp(a_1)
a_1_true <- exp(log(1.1))

mean(CLQI_MI_results) - a_1_true #bias... super low
```

***************************************************************************************************************

Now we are going to do some simulations to get a distribution for bias. Note the runtime

```{r}
sim_size <- 20
bias_results <- numeric(sim_size)
MSE_results <- numeric(sim_size)

a_1_true <- exp(log(1.1))

tictoc::tic()

# Use purrr to iterate over simulations
sim_results <- purrr::map_dfr(1:sim_size, ~ {
  
  CLQI_MI_results <- CLQI_MI(missing_dataset = missing_study, 
                             basis_coefficients = coefficient_data,
                             var_for_imputation = "confounder_1_missing")
  
  # Calculate bias and MSE for each iteration
  bias_val <- mean(CLQI_MI_results) - a_1_true
  MSE_val <- bias_val^2 + var(CLQI_MI_results)
  
  # Return values as a data frame for easy handling
  data.frame(bias = bias_val, MSE = MSE_val)
  }
)

ggplot(data=sim_results, aes(x=bias)) +
  geom_histogram(bins=5)
```

Coverage example: maybe should implement in the CLQI function

```{r}
loggy <- glm(outcome ~ exposure + predictor + confounder_1_CLQI, 
                          data = missing_study, family = "binomial")

summary(loggy)

#get the p-value? or maybe get the standard error


#indicator that we have a significant value or not...

```


***************************************************************************************************************

Make sure we implement the other imputation methods under the same setting! (especially fixed imputation)

Complete case: use `confounder_1_missing` and just ignore all NA values when doing regression

```{r}

```

Constant Imputation: 

```{r}
missing_study <- missing_study |>
  mutate(confounder_1_constant_imputed = ifelse(is.na(confounder_1_missing), sqrt(2), confounder_1_missing))
```

Fixed Imputation:

```{r}

```

***************************************************************************************************************

Save all this information to show later!

```{r}

```

*Use a new qmd document to do the other scenerios and clean everything up*
*Some of the code here really was for demonstration purposes*