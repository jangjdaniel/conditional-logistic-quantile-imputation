
This QMD will house the simulation study for Conditional Logistic Quantile Imputation (CLQI), comparing its performance measures to existing methods in the literature (Complete Case, Predictive Mean Matching - PMM)

CLQI's advantage is similar to PMM where we aim to avoid implausible values and reduce bias after imputation. However, PMM has several limitations, such as only performing well under 10%-30% missing data, relying on assumptions for linear regression, and the distribution for imputation having little to moderate skew only.


*Caveat*
- We will soon be simulating from an existing observational study with a biomarker. The current data generating mechanism is to show that CLQI is a more efficient method than PMM


# Initial Code: Must run everytime

```{r}
#necessary libraries
library(tidyverse)
library(ggplot2)
library(MASS)
library(purrr) #for looping
library(tictoc) #for checking runtime

library(quantreg) #for logistic quantile regression
library(missMethods) #general missing methods
library(mice) #for PMM

library(Rcpp) #future implementation for faster calculations in CLQI
```

Some important transformation `functions` that we will use for the CLQI algorithm:

```{r}
#logit and expit functions for myself
logit <- function(prob) {
  value <- log(prob / (1 - prob))
  return(value)
}

expit <- function(x) {
  value <- 1 / (1 + exp(-x))
  return(value)
}

#now some other transformation functions
log_quant_transform <- function(value, min, max) {
  if (is.na(value)) return(NA)  # short-circuit if missing
  if (value <= min | value >= max) return(NA)
  return(log((value - min) / (max - value)))
}

inv_log_quant_transform <- function(value, min, max) {
  new_value <- (exp(value)*max + min) / (1+exp(value))
  return(new_value)
}
```

# Data Generating Mechanism (DGM)

For our Data Generating Mechanism, we need a way to obtain quantile values for a mixture distribution with two chi square distributions. The function is specified below with two test cases

```{r}
get_mixture_quantile <- function(p) {
  uniroot(
    function(q) 0.6 * pchisq(q, 5) + 0.4 * pchisq(q, 8) - p, #this is specific to our DGM
    c(0, 1000)
  )$root
}

# Compute each quantile and assign to unique objects
q_9999 <- get_mixture_quantile(0.9999)
q_999999 <- get_mixture_quantile(0.999999)
```

Below is the function for our DGM, with distributions and coefficients specified in `Section 3`. 
The logic of this function is we first generate our data from specificed distributions

Some of the *MAR* functionality is not implemented yet

```{r}
data_generation <- function(sample_size, missing_prop, 
                            p_missing_zero = 0.35, p_missing_one = 0.20,
                            b_0 = logit(0.1), b_1 = log(1.1), b_2 = log(0.7),  b_3 = log(0.85)) {

  #generate my data
  B <- rbinom(sample_size, size = 1, prob = 0.4)
  V <- rnorm(sample_size, mean = 0, sd = 1) #this is our predictor
  X <- rchisq(sample_size, df = 5 + 3*B) #if B = 1, X ~ chisq(8)
  Y <- plogis(b_0 + b_1*X + b_2*B + b_3*V) #we have some effects
  Y_bin <- rbinom(sample_size, size = 1, prob = Y)
  
  #taking all these elements and putting them into a data frame
  my_data <- data.frame(
      confounder = B,
      predictor = V,
      biomarker = X,
      missing_biomarker = X,
      missing_biomarker_MAR = X,
      outcome = Y_bin
    )

  #make missing_prop percent of data missing MCAR
  my_data <- delete_MCAR(my_data, missing_prop, "missing_biomarker") 
  
#what about the MAR scenario?
  #get probabilities based on confounder and unif from 0 to 1 for all my datapoints
  prob <- ifelse(my_data$confounder == 1, p_missing_one, p_missing_zero)
  u <- runif(nrow(my_data))
  
  #apply MAR missingness
  my_data$missing_biomarker_MAR <- ifelse(u < prob, #if my u value is less than my probability
                                          NA, #make it missing
                                          my_data$missing_biomarker_MAR) #if not, keep the observation

#now we create the transformed variable... sapply is NOT working
  my_data$transformed_biomarker <- NA
  
  for(i in 1:nrow(my_data)) {
    if(is.na(my_data$missing_biomarker[i])) {
      my_data$transformed_biomarker[i] <- NA} 
    else {
      my_data$transformed_biomarker[i] <- log_quant_transform(my_data$missing_biomarker[i], 
                                                              min = 0, max = q_999999)}
  }
  
  return(my_data)
}
```

*We need to ensure that this value is consistent throughout the entire analysis. This is why there are multiple chunks for seemingly small things. We will describe in detail what these are*

We need to use the true effect defined in the DGM throughout the simulation study. 

```{r}
#our true effect
true_effect <- log(1.1)
```

For logistic regression throughout the simulation study, we will use these formulas for specific cases
`as.formula` is needed for this to work with lm objects

```{r}
#for CLQI, this quantile logistic relationship is what we need to generate imputed values
imputation_relationships <- "transformed_biomarker ~ outcome + confounder + predictor"

#in general, to check the true logistic regression for the data before missingness, we need this
full_relationships <- "outcome ~ biomarker + confounder + predictor"

#FOR PMM/CC: the reason why this is different is because of how the mice package works
missing_relationships_PMM <- "outcome ~ missing_biomarker + confounder + predictor"

#FOR CLQI
missing_relationships_CLQI <- "outcome ~ untransformed_imputed_biomarker + confounder + predictor"
```

For any test case we have, we will use this data only. This is for consistency

```{r}
#for any tests, you must take from testing_data_only
set.seed(500)
testing_data_only <- data_generation(sample_size = 1000, missing_prop = 0.3)
plogis(0.5)
```

Some plots of generated data as a `check`

```{r, eval=FALSE}
created_data <- testing_data_only

#FIRST CHECK: Logistic Regression 
regression_results <- glm(as.formula(full_relationships),
                          data = created_data,
                          family = "binomial")

summary(regression_results)$coefficients[2] - true_effect #what is the bias of the biomarker estimate?

# Convert confounder to a factor for better labeling and aesthetics
created_data$confounder <- factor(created_data$confounder, levels = c(0, 1), labels = c("Confounder = 0", "Confounder = 1"))

# Create the plot with overlaid density curves
ggplot(data = created_data, aes(x = biomarker, fill = confounder, color = confounder)) +
  geom_density(alpha = 0.5) + 
  labs(title = "Biomarker Distribution by Confounder",
       x = "Biomarker",
       y = "Density") +
  scale_fill_manual(values = c("skyblue", "orange")) +  # Custom colors for filling
  scale_color_manual(values = c("blue", "red"))        # Custom border colors
```

# Code for methods to be compared (PMM, CLQI)
## PMM: This is some simple code to demonstrate how PMM works with the `mice` package in R

```{r, eval=FALSE}
# set seed for reproducibility
set.seed(500)
num_imp <- 10 #number of imputations

# mice requires the dataframe to only contain missing variable and its predictors
created_data_for_PMM <- testing_data_only |>
  dplyr::select(confounder, missing_biomarker, outcome, predictor) 

# one imputation of PMM using mice: message suppression needed
imp <- mice(created_data_for_PMM, 
            method = "pmm", 
            m = num_imp) 

# perform multiple imputations and aggregating results with Rubin Rules (RR)
PMM_coefficients <- c()

for(i in 1:num_imp) {
  imputed_data <- complete(imp, action = i)
  my_reg <- glm(as.formula(missing_relationships_PMM),
                data = imputed_data,
                family = "binomial")
    
  PMM_coefficients[i] <- summary(my_reg)$coefficients[2]
}

# quick check on bias of one iteration
mean(PMM_coefficients) - true_effect
```

## CLQI: Perform the algorithm
*As a side note, I may use Rcpp to speed up calculations here*
*As another side note, the Warning is that the solution may be nonunique. This does not matter*

since we are imputing from the study, we do not need to calculate a weighted average.

Instead, we just run a unique logistic regression.

This can be extremely slow, but that is not the concern right now.

If we work with much larger datasets, then we can think of doing a weighted average, which would involve running 99 regressions and using those coefficients instead. 

This would be straightforward to code, but make sure to vectorize correctly!

```{r}
imputation_algorithm <- function(my_data, row_index) {
  
  #generate a random uniform value
  u <- runif(1, min = 0, max = 0.99) 
  
  #perform LQR 
  reg_coeff <- rq(as.formula(imputation_relationships), 
                        data = my_data, 
                        tau = u)
  
  #save all regression coefficients
  b_intercept <- reg_coeff$coefficients[1]
  b_outcome <- reg_coeff$coefficients[2]
  b_confounder <- reg_coeff$coefficients[3]
  b_predictor <- reg_coeff$coefficients[4]
  
  #predicted imputed value (which is in its transformed state)
  imputation_value_transformed <- b_intercept + (b_outcome * my_data[row_index,]$outcome) + (b_confounder * my_data[row_index,]$confounder) + (b_predictor * my_data[row_index,]$predictor)
  
  #return predicted value
  return(imputation_value_transformed)
}
```

### The following is a demonstration of CLQI with one example for imputation. This is *not* a full implementation with multiple imputations and aggregation with Rubin Rules.

```{r, eval=FALSE}
#generate our data with missing data for the biomarker
data_for_imp <- testing_data_only

#there may be a better way to do this, but for now:
#if our data is missing, we perform the algorithm
  for(row_index in 1:nrow(data_for_imp)) {
    if(is.na(data_for_imp$transformed_biomarker[row_index])) {
      
      imputed_value <- imputation_algorithm(my_data = data_for_imp,
                                            row_index = row_index)
      
      data_for_imp$transformed_biomarker[row_index] <- imputed_value
    }
  }

data_for_imp <- data_for_imp |>
  mutate(untransformed_imputed_biomarker = sapply(transformed_biomarker, inv_log_quant_transform, 
                                                  min = 0, max = q_999999))

data_for_imp #check the transformed_biomarker variable, not the missing_biomarker variable!
```

These are some visualizations for the imputation for CLQI. This is very messy code and is only here for demonstration purposes.

```{r, eval=FALSE}
data_for_imp_new <- data_for_imp

data_for_imp_new$confounder <- factor(data_for_imp_new$confounder, 
                                      levels = c(0, 1), 
                                      labels = c("Confounder = 0", "Confounder = 1"))

# Create the plot with overlaid density curves
ggplot(data = data_for_imp_new, aes(x = biomarker, fill = confounder, color = confounder)) +
  geom_density(alpha = 0.5) + 
  labs(title = "Biomarker Distribution by Confounder",
       x = "Biomarker",
       y = "Density") +
  scale_fill_manual(values = c("skyblue", "orange")) +  # Custom colors for filling
  scale_color_manual(values = c("blue", "red"))        # Custom border colors


ggplot(data = data_for_imp_new, aes(x = untransformed_imputed_biomarker, fill = confounder, color = confounder)) +
  geom_density(alpha = 0.5) + 
  labs(title = "Imputed Biomarker Distribution by Confounder",
       x = "Biomarker",
       y = "Density") +
  scale_fill_manual(values = c("skyblue", "orange")) +  # Custom colors for filling
  scale_color_manual(values = c("blue", "red")) +       # Custom border colors
  stat_function(fun = dchisq, args = list(df = 5), aes(color = NULL), linetype = "dashed", size = 1, color = "black") +
  stat_function(fun = dchisq, args = list(df = 8), aes(color = NULL), linetype = "dotted", size = 1, color = "darkgreen")
```

Lastly, a logistic regression test for *bias*

```{r, eval=FALSE}
CLQI_glm <- glm(as.formula(missing_relationships_CLQI),
              data = data_for_imp,
              family = "binomial")

summary(CLQI_glm)$coefficients[2] - true_effect
```

# Distance Metrics for Imputation Quality for PMM and CLQI

We can look at histograms with the imputed data and real data all we want, but there must be a way to quantify how different those two distributions are. For our purposes, we have *KS-test*, which is just the maximum vertical Euclidean distance between two distributions (or supremum if you're working with infinitesimals)

```{r, eval=FALSE}
#redo data generation
data_for_imp <- testing_data_only

#separate data for KS test
data_for_imp_zero <- data_for_imp |>
  filter(confounder == 0)

data_for_imp_one <- data_for_imp |>
  filter(confounder == 1)

#comparing with a theoretical chi square distribution from which the data was generated
ks.test(data_for_imp_zero$untransformed_imputed_biomarker, "pchisq", df = 5)
ks.test(data_for_imp_one$untransformed_imputed_biomarker, "pchisq", df = 8)
```

Now we will focus on *1-Wasserstein distance*, which measures the difference between the two distributions throughout their entire support.

```{r, eval=FALSE}
wasserstein_1_distance <- function(variable, my_df) {
  empirical_cdf <- ecdf(variable)
  u <- seq(0, 0.9999999, length.out = 1000) #we will be working with the quantile version. Integration approximation
  
  #now get the quantiles
  empirical_quantiles <- quantile(variable, probs = u)
  theoretical_quantiles <- qchisq(u, df = my_df)
  
  return(mean(as.numeric(abs(empirical_quantiles - theoretical_quantiles))))
}

wasserstein_1_distance(data_for_imp_zero$untransformed_imputed_biomarker, my_df = 5) #from DGM
wasserstein_1_distance(data_for_imp_one$untransformed_imputed_biomarker, my_df = 8) #ditto
```

################################################################################################################
# Simulation Study Functions

We defined a couple of functions to make everything much easier:

*rubin_rule_SE* takes in two vectors: one of the parameter values after performing imputation and one of its associated standard erros. This performs rubin rule calculations to produce the SE. As a reminder, the parameter value is given by taking the average of the `m` imputations

```{r}
#rubin's rule for standard error is complicated, so just keep it in a simple function
rubin_rule_SE <- function(values_MI, SE_MI) { #both inputs are vectors
  variance_within_MI <- mean((SE_MI)^2)
  variance_between_MI <- sum((values_MI - mean(values_MI))^2) / (length(values_MI) - 1)
  total_variance <- variance_within_MI + variance_between_MI + (variance_between_MI / length(values_MI))
  
  return(sqrt(total_variance))
}
```

*A test case has not been implemented, but it should before the final simulation study is done*

*coverage* gives a binary value, 1 if the true parameter is contained in our given CI using the *nominal* rate being 95%, and 0 if not. The logic here should make sense if you know what coverage is

```{r}
coverage <- function(parameter, SE, t_star, true_val) {
  binary <- ifelse(parameter - t_star*SE <= true_val & true_val <= parameter + t_star*SE, 
                   1, 0) #return 1 if true value is contained in CI
  
  return(binary)
}
```

*A test case has not been implemented, but it should before the final simulation study is done*

*power* gives a binary value.

We know that from our DGM that there is a true different effect, so our estimate should be statistically significant for a difference. How we do this is we calculate a `wald` statistic with a specific `degrees of freedom` given a vector of parameter estimates given from MI and their associated standard errors. If we reject the null with alpha = 0.05, then this function returns 1.

```{r}
power <- function(values_MI, SE_MI, num_MI) {
  #t stat calculation using wald test
  wald <- mean(values_MI) / rubin_rule_SE(values_MI, SE_MI)
  
  #df calculation
  variance_within_MI <- mean((SE_MI)^2)
  variance_between_MI <- sum((values_MI - mean(values_MI))^2) / (length(values_MI) - 1)
  my_frac <- variance_between_MI / (num_MI * variance_within_MI)
  df <- ((num_MI - 1) / (1 + my_frac)^2) 
  
  #our p-value
  p_value <- 2 * (1 - pt(abs(wald), df))
  
  #return 1 if we reject H0 (from DGM, we know H1 to be true)
  return(ifelse(p_value < 0.05, 1, 0))
}
```

*A test case has not been implemented, but it should before the final simulation study is done*

## Predictive Mean Matching

In this subsection, we will be defining the PMM function and documenting (in detail) what is going on

Caveat: This function is for one full implementation of the algorithm. For a simulation study, this will need to be repeated N times

```{r}
PMM <- function(my_data, num_MI_iter) {
  suppressMessages({
  suppressWarnings({
  capture.output({ #the mice package has weird output that isn't relevant for us. This is largely ignored
        
     #initialize vectors that we know we need, with corresponding sizes
      values_PMM <- c()
      SE_PMM <- c()
      
  
     #creates m number of datasets and imputes them based on 
      imp <- mice(my_data, method = "pmm", m = num_MI_iter)
        
      # the for loop to extract values and SE
      for (j in 1:num_MI_iter) { #for each imputed dataset
        imputed_data <- complete(imp, action = j)
        
        my_reg <- glm(as.formula(missing_relationships_PMM), #run logistic regression
                      data = imputed_data,
                      family = "binomial")
          
        # store these values
        values_PMM[j] <- summary(my_reg)$coefficients["missing_biomarker", "Estimate"]
        SE_PMM[j] <- summary(my_reg)$coefficients["missing_biomarker", "Std. Error"]
      }
        
      #return the necessary parameter estimates and SE as a list for analysis!
      results_PMM <- list(values_PMM, SE_PMM)
        
      return(results_PMM)
        
  }) 
  }) 
  })
}
```

Test case for PMM:

```{r}
testing_data_for_PMM <- testing_data_only |>
  dplyr::select(confounder, missing_biomarker, outcome, predictor) #must do this as per mice package rules

  tictoc::tic()
a <- PMM(testing_data_for_PMM, num_MI_iter = 10)
  tictoc::toc()
mean(as.vector(a[[1]]) - true_effect)
```

## CLQI

The following is a functional implementation of the CLQI algorithm. Like above, this is for one full implementation of CLQI, so simulations will need to repeat this function N times.

Note that this function requires the use of `imputation_algorithm`

I know nested for loops are not great, but I'm not changing the world with this. If I work with genomic data with this, then I'll change it.

```{r}
CLQI <- function(my_data, num_MI_iter) {
  
  #initialize vectors that will be of use for us
  values_CLQI <- c()
  SE_CLQI <- c()
  
  #multiple part of the multiple imputation
  for(imp in 1:num_MI_iter) {
    my_data_iteration <- my_data #resets any imputation that happened
    
    for(row_index in 1:nrow(my_data_iteration)) {
      if(is.na(my_data_iteration$transformed_biomarker[row_index])) {
        imputed_value <- imputation_algorithm(my_data = my_data_iteration,
                                              row_index = row_index)
        
        my_data_iteration$transformed_biomarker[row_index] <- imputed_value
      }
    }
  
  #after running the algorithm, we untransform, perform log reg, save coefficients, rinse and repeat
  my_data_iteration <- my_data_iteration |>
  mutate(untransformed_imputed_biomarker = sapply(transformed_biomarker, inv_log_quant_transform, 
                                                  min = 0, max = q_999999))

  my_glm <- glm(as.formula(missing_relationships_CLQI),
              data = my_data_iteration,
              family = "binomial")

  values_CLQI[imp] <- summary(my_glm)$coefficients[2]
  SE_CLQI[imp] <- summary(my_glm)$coefficients[2,2]
  } 
  
  CLQI_results <- list(values_CLQI, SE_CLQI)
  return(CLQI_results)
}
```

Again, another test case!

```{r}
testing_data_for_CLQI <- testing_data_only

  tictoc::tic()
b <- CLQI(testing_data_for_CLQI, num_MI_iter = 10)
  tictoc::toc()
  
mean(b[[1]] - true_effect)
```

## Complete Case (CC) Analysis 

Quick functional implemention of complete case analysis just for us!

```{r}
complete_case <- function(my_data) {
  my_glm <- glm(as.formula(missing_relationships_PMM), #this works with CC as well because of how I coded everything
              data = my_data,
              family = "binomial")
  
  value_CC <- summary(my_glm)$coefficients[2]
  SE_CC <- summary(my_glm)$coefficients[2,2]
  pval_CC <- summary(my_glm)$coefficients["missing_biomarker", "Pr(>|z|)"]
  
  return(c(value_CC, SE_CC, pval_CC))
}

testing_data_for_CC <- testing_data_only

c <- complete_case(testing_data_for_CC)
c[1] #the estimate
```

Now we will combine all of these functions to create a huge simulation function that just combines everything we did above into one

```{r}
simulation_study_function <- function(sample_size, missing_prop, num_sim, MI_iter) {
  #here are some vectors we have to define... this is still messy since it's not in a full function
  estimate_values_PMM <- c()
  SE_values_PMM <- c()
  coverage_PMM <- c()
  power_PMM <- c()
  
  estimate_values_CLQI <- c()
  SE_values_CLQI <- c()
  coverage_CLQI <- c()
  power_CLQI <- c()
  
  estimate_values_CC <- c()
  SE_values_CC <- c()
  coverage_CC <- c()
  power_CC <- c()
    
  my_iter_count <- 0 #this is here to make sure my code is running, because sometimes it wasn't
  t_star <- qt(0.975, df = sample_size - 1) #nomial value for coverage stuff
  
  for(i in 1:num_sim) { #for each simulation
    data_for_imp <- data_generation(sample_size = sample_size, 
                                    missing_prop = missing_prop) #generate my data
####  
    #PMM:
    #get values for estimate and SE from PMM
    data_for_imp_PMM <- data_for_imp  |>
      dplyr::select(confounder, missing_biomarker, outcome, predictor)
    
    PMM_results <- PMM(data_for_imp_PMM, num_MI_iter = MI_iter) #1 is estimates, 2 is SE
      values_PMM <- as.vector(PMM_results[[1]])
      SE_PMM <- as.vector(PMM_results[[2]])
  
    #perform calculations to get estimate, bias, relative bias, RMSE, coverage, power for PMM
      estimate_values_PMM[i] <- mean(values_PMM)
      SE_values_PMM[i] <- rubin_rule_SE(values_PMM, SE_PMM)
      coverage_PMM[i] <- coverage(parameter = estimate_values_PMM[i],
                                  SE = SE_values_PMM[i],
                                  t_star = t_star,
                                  true_val = true_effect)
      power_PMM[i] <- power(values_PMM, SE_PMM, MI_iter)
      
####
    #get values for estimate and SE from CLQI
    CLQI_results <- CLQI(data_for_imp, num_MI_iter = MI_iter) #1 is estimates, 2 is SE
      values_CLQI <- as.vector(CLQI_results[[1]])
      SE_CLQI <- as.vector(CLQI_results[[2]])

    #perform calculations to get estimate, bias, relative bias, RMSE, coverage, power for CLQI
        estimate_values_CLQI[i] <- mean(values_CLQI)
        SE_values_CLQI[i] <- rubin_rule_SE(values_CLQI, SE_CLQI)
        coverage_CLQI[i] <- coverage(parameter = estimate_values_CLQI[i],
                                     SE = SE_values_CLQI[i],
                                     t_star = t_star,
                                     true_val = true_effect)
        power_CLQI[i] <- power(values_CLQI, SE_CLQI, MI_iter)
#### 
    #perform complete case analysis
    complete_case_results <- complete_case(data_for_imp)
      
    #perform calculations to get estimate, bias, relative bias, RMSE, coverage, power for CC
        estimate_values_CC[i] <- complete_case_results[1]
        SE_values_CC[i] <- complete_case_results[1]
        coverage_CC[i] <- coverage(parameter = estimate_values_CC[i],
                                   SE = SE_values_CC[i],
                                   t_star = t_star,
                                   true_val = true_effect)
        power_CC[i] <- ifelse(as.numeric(complete_case_results[3]) < 0.05, 1, 0) #if p-val < 0.05, it's 1. Else, it's 0.
      
  #we will be repeating this 1000 times
  my_iter_count <- my_iter_count + 1
  print(my_iter_count)
  }
  
  #AFTER FOR LOOP WE NEED TO CREATE THE DATASET
  
    #create bias rows
    bias_values_PMM <- estimate_values_PMM - true_effect
    bias_values_CLQI <- estimate_values_CLQI - true_effect
    bias_values_CC <- estimate_values_CC - true_effect
    
    #and at the end, combine all these vectors into a single dataframe
    my_simulation_results <- data.frame(
      estimate_values_PMM = estimate_values_PMM,
      bias_values_PMM = bias_values_PMM,
      SE_values_PMM = SE_values_PMM,
      RMSE_values_PMM = sqrt(bias_values_PMM^2 + SE_values_PMM^2),
      coverage_PMM = coverage_PMM,
      power_PMM = power_PMM, #done
      estimate_values_CLQI = estimate_values_CLQI,
      bias_values_CLQI = bias_values_CLQI,
      SE_values_CLQI = SE_values_CLQI,
      RMSE_values_CLQI = sqrt(bias_values_CLQI^2 + SE_values_CLQI^2),
      coverage_CLQI = coverage_CLQI,
      power_CLQI = power_CLQI, #done
      estimate_values_CC= estimate_values_CC,
      bias_values_CC= bias_values_CC,
      SE_values_CC = SE_values_CC,
      RMSE_values_CC = sqrt(bias_values_CC^2 + SE_values_CC^2),
      coverage_CC = coverage_CC,
      power_CC = power_CC
    )
    
  return(my_simulation_results)
}
```

Test case with 10 simulations

```{r, eval=FALSE}
tictoc::tic()
simulation_study_function(sample_size = 1000,
                          missing_prop = 0.3,
                          num_sim = 10,
                          MI_iter = 10)
tictoc::toc()
```


##############################################################################################################################

# Simulation Study

Before we start our simulation study, we need to define the simulation settings we will perform. 

Here is the information that we will be keeping consistent throughout each setting

```{r}
set.seed(500) #not sure why this wouldn't be implemented, but just in case
num_sim <- 1000 #number of simulations: will probably bump to 5000, but I need to make my code more efficient maybe
MI_iter <- 10 #number of iterations: will likely bump to 25 
```

*SIMULATION 1*
For now, we only have one setting to test. 

```{r}
tictoc::tic()
simulation_study_setting_1 <- simulation_study_function(sample_size = 1000,
                                                        missing_prop = 0.3,
                                                        num_sim = num_sim,
                                                        MI_iter = MI_iter)
tictoc::toc()
```


```{r}
#quick bias checks 
(mean(my_simulation_results_q_999999$estimate_values_CLQI) - true_effect) / true_effect * 100
(mean(my_simulation_results_q_999999$estimate_values_PMM) - true_effect) / true_effect * 100
(mean(my_simulation_results_q_999999$estimate_values_CC) - true_effect) / true_effect * 100

#now for RMSE
mean(my_simulation_results_q_999999$RMSE_values_CLQI)
mean(my_simulation_results_q_999999$RMSE_values_PMM)
mean(my_simulation_results_q_999999$RMSE_values_CC)

#now for coverage
mean(my_simulation_results_q_999999$coverage_CLQI)
mean(my_simulation_results_q_999999$coverage_PMM)
mean(my_simulation_results_q_999999$coverage_CC)

#now for power
mean(my_simulation_results_q_999999$power_CLQI)
mean(my_simulation_results_q_999999$power_PMM)
mean(my_simulation_results_q_999999$power_CC)

#save this!
saveRDS(my_simulation_results_q_999999, file="./simulation_study_setting_1.rds")
```

Everything is sad!!!